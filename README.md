# pinot-docker-compose
## Getting Started
Run `docker compose up -d` to start all Pinot services. The following components will be provisioned:
- zookeeper, host.docker.internal:22181
- kafka, host.docker.internal:29092
- kafka-ui, host.docker.internal:29080, http://host.docker.internal:29080
- pinot controller, host.docker.internal:29000, http://host.docker.internal:29000
- pinot server, 28098, host.docker.internal:28097
- pinot broker, host.docker.internal:28099

After all components are provisioned, let's go through how to create a Pinot table that ingests records from Kafka and also supports upsert.

### Create Kafka Topic
The Kafka topic should be created before Pinot table created, let's open http://localhost:29080/ui/clusters/local/topics and create a topic such as `togai-event-store`.

### Create Pinot Schema
Each Pinot table is associated with a Pinot schema, a Pinot schema can be shared with multiple tables.

Here is the schema definition:
```json
{
  "schemaName": "togai_event_store",
  "dimensionFieldSpecs": [
    {
      "name": "organizationId",
      "dataType": "STRING"
    },
    {
      "name": "accountId",
      "dataType": "STRING"
    },
    {
      "name": "schemaName",
      "dataType": "STRING"
    },
    {
      "name": "eventId",
      "dataType": "STRING"
    },
    {
      "name": "payload",
      "dataType": "JSON",
      "maxLength": 102400
    },
    {
      "name": "metadata",
      "dataType": "STRING"
    },
    {
      "name": "ingestionStatus",
      "dataType": "INT"
    },
    {
      "name": "ingestionStatusDesc",
      "dataType": "STRING"
    },
    {
      "name": "customerId",
      "dataType": "STRING"
    },
    {
      "name": "sourceId",
      "dataType": "STRING"
    },
    {
      "name": "sourceType",
      "dataType": "STRING"
    },
    {
      "name": "id",
      "dataType": "STRING"
    }
  ],
  "dateTimeFieldSpecs": [
    {
      "name": "eventSourceTime",
      "dataType": "STRING",
      "format": "SIMPLE_DATE_FORMAT|yyyy-MM-dd'T'HH:mm:ss'Z'",
      "granularity": "1:MILLISECONDS"
    },
    {
      "name": "statusUpdatedTime",
      "dataType": "STRING",
      "format": "SIMPLE_DATE_FORMAT|yyyy-MM-dd'T'HH:mm:ss'Z'",
      "granularity": "1:MILLISECONDS"
    },
    {
      "name": "createdAt",
      "dataType": "STRING",
      "format": "SIMPLE_DATE_FORMAT|yyyy-MM-dd'T'HH:mm:ss'Z'",
      "granularity": "1:MILLISECONDS"
    },
    {
      "name": "ingestionTime",
      "dataType": "LONG",
      "format": "1:MILLISECONDS:EPOCH",
      "granularity": "1:MILLISECONDS"
    },
    {
      "name": "uploadTime",
      "dataType": "LONG",
      "format": "1:MILLISECONDS:EPOCH",
      "granularity": "1:MILLISECONDS"
    }
  ],
  "primaryKeyColumns": [
    "eventId"
  ]
}
```

The `primaryKeyColumns` is required for upsert table. You can check [Schema](https://docs.pinot.apache.org/configuration-reference/schema) for more details.

You can open [Pinot Controller API](http://localhost:29000/help) and use `POST /schemas` to create a schema (no auth required).

### Create Pinot Table
Now we can create an upsert table with the table definition:
```json
{
  "tableName": "togai_event_store",
  "tableType": "REALTIME",
  "segmentsConfig": {
    "timeType": "MILLISECONDS",
    "schemaName": "togai_event_store",
    "retentionTimeUnit": "DAYS",
    "retentionTimeValue": "30",
    "replicasPerPartition": "1",
    "minimizeDataMovement": false,
    "timeColumnName": "ingestionTime",
    "completionConfig": {
      "completionMode": "DOWNLOAD"
    },
    "segmentPushType": "APPEND"
  },
  "tenants": {
    "broker": "DefaultTenant",
    "server": "DefaultTenant"
  },
  "tableIndexConfig": {
    "invertedIndexColumns": [
      "organizationId",
      "accountId",
      "schemaName",
      "eventId"
    ],
    "noDictionaryColumns": [
      "payload",
      "metadata"
    ],
    "streamConfigs": {
      "streamType": "kafka",
      "stream.kafka.consumer.type": "lowlevel",
      "stream.kafka.topic.name": "togai-event-store",
      "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",
      "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
      "stream.kafka.broker.list": "kafka:9092",
      "stream.kafka.consumer.prop.auto.offset.reset": "largest",
      "stream.kafka.isolation.level": "read_committed",
      "realtime.segment.flush.threshold.time": "12h",
      "realtime.segment.flush.threshold.rows": "0",
      "realtime.segment.flush.threshold.segment.size": "200M",
      "realtime.segment.flush.autotune.initialRows": "100000"
    },
    "loadMode": "MMAP",
    "enableDefaultStarTree": false,
    "starTreeIndexConfigs": [],
    "enableDynamicStarTreeCreation": false,
    "rangeIndexColumns": [
      "ingestionTime",
      "eventSourceTime"
    ],
    "rangeIndexVersion": 2,
    "autoGeneratedInvertedIndex": false,
    "createInvertedIndexDuringSegmentGeneration": false,
    "sortedColumn": [
      "customerId"
    ],
    "aggregateMetrics": false,
    "nullHandlingEnabled": true,
    "optimizeDictionary": false,
    "optimizeDictionaryForMetrics": true,
    "noDictionarySizeRatioThreshold": 0
  },
  "metadata": {
    "customConfigs": {}
  },
  "routing": {
    "instanceSelectorType": "strictReplicaGroup"
  },
  "upsertConfig": {
    "mode": "FULL",
    "comparisonColumns": [
      "ingestionTime"
    ],
    "hashFunction": "MURMUR3",
    "enableSnapshot": false,
    "defaultPartialUpsertStrategy": "OVERWRITE",
    "metadataTTL": 0,
    "enablePreload": false
  },
  "ingestionConfig": {
    "filterConfig": {},
    "continueOnError": false,
    "transformConfigs": [
      {
        "columnName": "ingestionTime",
        "transformFunction": "now()"
      }
    ],
    "rowTimeValueCheck": false,
    "segmentTimeValueCheck": true
  },
  "isDimTable": false
}
```

`upsertConfig` describes the behavior of upsert, you can check [Stream ingestion with Upsert](https://docs.pinot.apache.org/basics/data-import/upsert) for more details about upsert config.

### Ingest Usage
Let's send the following message with `eventId` as partition key to Kafka:
```json
{
	"sourceId": null,
	"eventId": "c35886c3-97ec-483f-9baf-601d4f91d1d9",
	"metadata": "\\x1f8b08000000000000ff7550cb4ec33010fc973d27561e45a0dc8022950a51e0c0a35555458e2387c64914db4455947f67378f1e4a387966763cfb684140d48286081438901029909826cbb3f250c9b210a8ff40e4779d03f6c2bc6b214366150bc22bfd107edc1d1f57ecb5a93e251a2e830e719e8f614e9fc291297a4b558d092f49bace5892d6f209fa7e61b7c7c6d5dfc63363d22cd590b44ef8f25d6cd9f7b239de6349534d73d91737cf7273fb5532ed9bb7108b9c92032f58b8deb5ebf9a488b372332856a16937bfeabe9f319ebde4e924dc98f3d216067a0f2984264877e5ff7d1d979a7c42510d7f1636cfd1999d9118107a523589b59a62ed78351a13b9a9ad406026a3966364f70b58e56b6610020000",
	"ingestionStatusDesc": "INGESTION_COMPLETED_EVENT_METERED",
	"ingestionStatus": 6,
	"eventSourceTime": "2024-06-16T02:15:56Z",
	"schemaName": "twilio_phone",
	"uploadTime": "1721359393562",
	"organizationId": "D6uYponVyD",
	"createdAt": "2024-07-18T07:28:40.398Z",
	"accountId": "TWT0749190",
	"sourceType": null,
	"payload": {
		"accountId": "TWT0749190",
		"attributes": [
			{
				"name": "aggregate",
				"value": "1000",
				"unit": "InMonth"
			}
		],
		"id": "c35886c3-97ec-483f-9baf-601d4f91d1d9",
		"schemaName": "togai_phone",
		"timestamp": "2024-06-16T02:15:56Z",
		"dimensions": {
			"uom": "uom1",
			"aggregationType": "AGG-TYPE-3",
			"aggregationKey": "AGG-KEY-27",
			"chargeName": "PUP_Twilio_7",
			"period": "2023-01-15"
		}
	},
	"customerId": "TWT0749190",
	"statusUpdatedTime": "2024-07-18T07:28:40.398Z",
	"id": "event.kqwsiucp5ya.wg1dh"
}
```

You can run `select * from togai_event_store limit 10` in Pinot's Query Console to check if the message is ingested.

Then change `ingestionStatus` to 1 to simulate the update case, and re-send the message to Kafka again. The `ingestionStatus` saved in Pinot should also be changed.

### Move Realtime Segments to Offline
We can periodically move realtime segments to offline, especially for the data that will be less frequently accessed.

You can simply add a `task` property to table config, this will be auto scheduled by Pinot:
```json
{
  "tableName": "togai_event_store",
  "tableType": "REALTIME",
  ...
  "task": {
    "taskTypeConfigsMap": {
      "RealtimeToOfflineSegmentsTask": {
        "bucketTimePeriod": "6h",
        "bufferTimePeriod": "5d",
        "roundBucketTimePeriod": "1h",
        "mergeType": "concat",
        "maxNumRecordsPerSegment": "100000"
      }
    }
  }
}
```

The above task moves 6h (`bucketTimePeriod`) data from realtime segments to offline, but tolerates 5d (`bufferTimePeriod`) delay.

You can check [Pinot managed Offline flows](https://docs.pinot.apache.org/operators/operating-pinot/pinot-managed-offline-flows) for more details.
